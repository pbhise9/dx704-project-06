{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 6 Project\n",
        "\n",
        "This project will develop a treatment plan for a fictious illness \"Twizzleflu\".\n",
        "Twizzleflu is a mild illness caused by a virus.\n",
        "The main symptoms are a mild fever, fidgeting, and kicking the blankets off the bed or couch.\n",
        "Mild dehydration has also been reported in more severe cases.\n",
        "These symptoms typically last 1-2 weeks without treatment.\n",
        "Word on the internet says that Twizzleflu can be cured faster by drinking copious orange juice, but this has not been supported by evidence so far.\n",
        "You will be provided with a theoretical model of Twizzleflu modeled as a Markov decision process.\n",
        "Based on the model, you will compute optimal treatment plans to optimize different criteria, and compare patient discomfort with the different plans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzyRo9Tw5VcB"
      },
      "source": [
        "The full project description, a template notebook, and raw data are available on GitHub: [Project 6 Materials](https://github.com/bu-cds-dx704/dx704-project-06)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGYOZcnP6Vfu"
      },
      "source": [
        "We will model Twizzleflu as a Markov decision process.\n",
        "The model transition probabilities are provided in the file \"twizzleflu-transitions.tsv\" and the expected rewards are in \"twizzleflu-rewards.tsv\".\n",
        "The goal for Twizzleflu is to minimize the expected discomfort of the patient which is expressed as negative rewards in the file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlm2sUsades5"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Evaluate a Do Nothing Plan\n",
        "\n",
        "One of the treatment actions is to do nothing.\n",
        "Calculate the expected discomfort (not rewards) of a policy that always does nothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvG4mi_sAF9A"
      },
      "source": [
        "Hint: for this value calculation and later ones, use value iteration.\n",
        "The analytical solution has difficulties in practice when there is no discount factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RVfnE8vf8yIl"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def pick(opts, cols):\n",
        "    for c in opts:\n",
        "        if c in cols:\n",
        "            return c\n",
        "    raise ValueError(f\"Missing one of {opts}; found {list(cols)}\")\n",
        "\n",
        "trans = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "rew0  = pd.read_csv(\"twizzleflu-rewards.tsv\", sep=\"\\t\")\n",
        "\n",
        "trans.columns = [c.strip().lower() for c in trans.columns]\n",
        "rew0.columns  = [c.strip().lower() for c in rew0.columns]\n",
        "\n",
        "s_col  = pick([\"state\",\"s\"], trans.columns)\n",
        "a_col  = pick([\"action\",\"a\"], trans.columns)\n",
        "sp_col = pick([\"next_state\",\"next\",\"sprime\",\"s_next\",\"sp\"], trans.columns)\n",
        "p_col  = pick([\"prob\",\"probability\",\"p\"], trans.columns)\n",
        "\n",
        "trans = trans[[s_col, a_col, sp_col, p_col]].copy()\n",
        "trans[s_col]  = trans[s_col].astype(str)\n",
        "trans[a_col]  = trans[a_col].astype(str)\n",
        "trans[sp_col] = trans[sp_col].astype(str)\n",
        "trans[p_col]  = trans[p_col].astype(float)\n",
        "\n",
        "states = sorted(set(trans[s_col]).union(set(trans[sp_col])))\n",
        "\n",
        "group = {}\n",
        "actions_by_state = {}\n",
        "for (s, a), df in trans.groupby([s_col, a_col]):\n",
        "    group[(s, a)] = list(zip(df[sp_col].tolist(), df[p_col].tolist()))\n",
        "    actions_by_state.setdefault(s, set()).add(a)\n",
        "\n",
        "def build_reward_fn(rew_df):\n",
        "    r = rew_df.copy()\n",
        "    r.columns = [c.strip().lower() for c in r.columns]\n",
        "    if \"reward\" not in r.columns:\n",
        "        raise ValueError(\"Rewards must have a 'reward' column.\")\n",
        "\n",
        "    if \"next_state\" not in r.columns:\n",
        "        for alt in [\"next\",\"sprime\",\"s_next\",\"sp\"]:\n",
        "            if alt in r.columns:\n",
        "                r = r.rename(columns={alt:\"next_state\"})\n",
        "                break\n",
        "\n",
        "    r_sas = None\n",
        "    r_sa  = None\n",
        "    r_s   = None\n",
        "\n",
        "    if (\"state\" in r.columns) and (\"action\" in r.columns) and (\"next_state\" in r.columns):\n",
        "        r_sas = {(str(row[\"state\"]), str(row[\"action\"]), str(row[\"next_state\"])): float(row[\"reward\"])\n",
        "                 for _, row in r.iterrows()}\n",
        "    elif (\"state\" in r.columns) and (\"action\" in r.columns):\n",
        "        r_sa = {(str(row[\"state\"]), str(row[\"action\"])): float(row[\"reward\"])\n",
        "                for _, row in r.iterrows()}\n",
        "    elif \"state\" in r.columns:\n",
        "        r_s = {str(row[\"state\"]): float(row[\"reward\"]) for _, row in r.iterrows()}\n",
        "    else:\n",
        "        raise ValueError(\"Rewards must contain at least 'state'.\")\n",
        "\n",
        "    def reward(s, a, sp):\n",
        "        if r_sas is not None:\n",
        "            return r_sas.get((s, a, sp), 0.0)\n",
        "        if r_sa is not None:\n",
        "            return r_sa.get((s, a), 0.0)\n",
        "        return r_s.get(sp, r_s.get(s, 0.0))\n",
        "\n",
        "    return reward\n",
        "\n",
        "reward_discomfort = build_reward_fn(rew0)\n",
        "\n",
        "\n",
        "def evaluate_policy_value(policy_map, reward_fn, gamma=0.99, tol=1e-10, max_iter=200000):\n",
        "    V = {s: 0.0 for s in states}\n",
        "    for _ in range(max_iter):\n",
        "        delta = 0.0\n",
        "        Vnew = {}\n",
        "        for s in states:\n",
        "            a = policy_map.get(s)\n",
        "            if a is None or (s, a) not in group:\n",
        "                Vnew[s] = V[s]\n",
        "                continue\n",
        "            v = 0.0\n",
        "            for sp, p in group[(s, a)]:\n",
        "                v += p * (reward_fn(s, a, sp) + gamma * V.get(sp, 0.0))\n",
        "            Vnew[s] = v\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        V = Vnew\n",
        "        if delta < tol:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def value_iteration_optimal_policy(reward_fn, gamma=0.99, tol=1e-10, max_iter=200000):\n",
        "    V = {s: 0.0 for s in states}\n",
        "    for _ in range(max_iter):\n",
        "        delta = 0.0\n",
        "        Vnew = {}\n",
        "        for s in states:\n",
        "            acts = actions_by_state.get(s)\n",
        "            if not acts:\n",
        "                Vnew[s] = V[s]\n",
        "                continue\n",
        "            best = -1e300\n",
        "            for a in acts:\n",
        "                v = 0.0\n",
        "                for sp, p in group[(s, a)]:\n",
        "                    v += p * (reward_fn(s, a, sp) + gamma * V.get(sp, 0.0))\n",
        "                if v > best:\n",
        "                    best = v\n",
        "            Vnew[s] = best\n",
        "            delta = max(delta, abs(best - V[s]))\n",
        "        V = Vnew\n",
        "        if delta < tol:\n",
        "            break\n",
        "\n",
        "    policy = {}\n",
        "    for s in states:\n",
        "        acts = actions_by_state.get(s)\n",
        "        if not acts:\n",
        "            continue\n",
        "        best_a = None\n",
        "        best_q = -1e300\n",
        "        for a in acts:\n",
        "            q = 0.0\n",
        "            for sp, p in group[(s, a)]:\n",
        "                q += p * (reward_fn(s, a, sp) + gamma * V.get(sp, 0.0))\n",
        "            if q > best_q:\n",
        "                best_q = q\n",
        "                best_a = a\n",
        "        policy[s] = best_a\n",
        "    return policy, V\n",
        "\n",
        "# part 1\n",
        "policy_do_nothing = {s: \"do-nothing\" for s in states}\n",
        "V_dn = evaluate_policy_value(policy_do_nothing, reward_discomfort, gamma=0.99)\n",
        "do_nothing_discomfort = {s: -V_dn[s] for s in states}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oji9gHEk8ytE"
      },
      "source": [
        "Save the expected discomfort by state to a file \"do-nothing-discomfort.tsv\" with columns state and expected_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZLDuiAb99ACA"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "pd.DataFrame({\"state\": states,\n",
        "              \"expected_discomfort\": [do_nothing_discomfort[s] for s in states]}\n",
        ").to_csv(\"do-nothing-discomfort.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-8sGANC-Dzs"
      },
      "source": [
        "Submit \"do-nothing-discomfort.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ1ietVp9BCS"
      },
      "source": [
        "## Part 2: Compute an Optimal Treatment Plan\n",
        "\n",
        "Compute an optimal treatment plan for Twizzleflu.\n",
        "It should minimize the expected discomfort (maximize the rewards)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6fdjt6qk9mZM"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "\n",
        "#trans = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "#rew = pd.read_csv(\"twizzleflu-rewards.tsv\", sep=\"\\t\")\n",
        "\n",
        "#policy_discomfort, _ = value_iteration_optimal_policy(reward_discomfort, gamma=0.99)\n",
        "\n",
        "gamma = 1.0\n",
        "tol = 1e-12\n",
        "max_iter = 200000\n",
        "\n",
        "def pick(opts, cols):\n",
        "    for c in opts:\n",
        "        if c in cols:\n",
        "            return c\n",
        "    raise ValueError(f\"Missing one of {opts}; found {list(cols)}\")\n",
        "\n",
        "s_col  = pick([\"state\",\"s\"], trans.columns)\n",
        "a_col  = pick([\"action\",\"a\"], trans.columns)\n",
        "sp_col = pick([\"next_state\",\"next\",\"sprime\",\"s_next\",\"sp\"], trans.columns)\n",
        "p_col  = pick([\"prob\",\"probability\",\"p\"], trans.columns)\n",
        "\n",
        "trans[s_col]  = trans[s_col].astype(str)\n",
        "trans[a_col]  = trans[a_col].astype(str)\n",
        "trans[sp_col] = trans[sp_col].astype(str)\n",
        "trans[p_col]  = trans[p_col].astype(float)\n",
        "\n",
        "states = sorted(set(trans[s_col]).union(set(trans[sp_col])))\n",
        "\n",
        "group = {}\n",
        "actions_by_state = {}\n",
        "for (s, a), df in trans.groupby([s_col, a_col]):\n",
        "    group[(s, a)] = list(zip(df[sp_col].tolist(), df[p_col].tolist()))\n",
        "    actions_by_state.setdefault(s, set()).add(a)\n",
        "\n",
        "def is_terminal_state(s):\n",
        "    acts = actions_by_state.get(s, None)\n",
        "    return (not acts)\n",
        "\n",
        "V = {s: 0.0 for s in states}\n",
        "\n",
        "for _ in range(max_iter):\n",
        "    delta = 0.0\n",
        "    Vnew = {}\n",
        "    for s in states:\n",
        "        if is_terminal_state(s):\n",
        "            Vnew[s] = 0.0\n",
        "            continue\n",
        "        best = -1e300\n",
        "        for a in actions_by_state[s]:\n",
        "            v = 0.0\n",
        "            for sp, p in group[(s, a)]:\n",
        "                v += p * (reward_discomfort(s, a, sp) + gamma * V.get(sp, 0.0))\n",
        "            if v > best:\n",
        "                best = v\n",
        "        Vnew[s] = best\n",
        "        delta = max(delta, abs(best - V[s]))\n",
        "    V = Vnew\n",
        "    if delta < tol:\n",
        "        break\n",
        "\n",
        "policy_discomfort = {}\n",
        "for s in states:\n",
        "    acts = actions_by_state.get(s, None)\n",
        "    if not acts:\n",
        "        continue\n",
        "    best_a = None\n",
        "    best_q = -1e300\n",
        "    for a in acts:\n",
        "        q = 0.0\n",
        "        for sp, p in group[(s, a)]:\n",
        "            q += p * (reward_discomfort(s, a, sp) + gamma * V.get(sp, 0.0))\n",
        "        if q > best_q:\n",
        "            best_q = q\n",
        "            best_a = a\n",
        "    policy_discomfort[s] = best_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRcByl1h9nBf"
      },
      "source": [
        "Save the optimal actions for each state to a file \"minimum-discomfort-actions.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FhAajvpX9wru"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "pd.DataFrame({\"state\": states,\n",
        "              \"action\": [policy_discomfort.get(s, \"\") for s in states]}\n",
        ").to_csv(\"minimum-discomfort-actions.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr00MlhL-Hdv"
      },
      "source": [
        "Submit \"minimum-discomfort-actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65p3NRTy9xjT"
      },
      "source": [
        "## Part 3: Expected Discomfort\n",
        "\n",
        "Using your previous optimal policy, compute the expected discomfort for each state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t5bHbK24-AhQ"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "Pi = pd.read_csv(\"minimum-discomfort-actions.tsv\", sep=\"\\t\")\n",
        "Pi_map = dict(zip(Pi[\"state\"].astype(str), Pi[\"action\"].astype(str)))\n",
        "\n",
        "def terminal_under_policy(s):\n",
        "    a = Pi_map.get(s, None)\n",
        "    if a is None or (s, a) not in group:\n",
        "        return True\n",
        "    nxt = group[(s, a)]\n",
        "    if len(nxt) == 1 and nxt[0][0] == s and abs(nxt[0][1] - 1.0) < 1e-12 and reward_discomfort(s, a, s) == 0.0:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "nonterm = [s for s in states if not terminal_under_policy(s)]\n",
        "term = [s for s in states if terminal_under_policy(s)]\n",
        "\n",
        "idx = {s: i for i, s in enumerate(nonterm)}\n",
        "n = len(nonterm)\n",
        "\n",
        "A = np.zeros((n, n), dtype=float)\n",
        "b = np.zeros(n, dtype=float)\n",
        "\n",
        "for s in nonterm:\n",
        "    i = idx[s]\n",
        "    a = Pi_map[s]\n",
        "    A[i, i] = 1.0\n",
        "    rhs = 0.0\n",
        "    for sp, p in group[(s, a)]:\n",
        "        rhs += p * reward_discomfort(s, a, sp)\n",
        "        if sp in idx:\n",
        "            A[i, idx[sp]] -= p  # gamma=1\n",
        "    b[i] = rhs\n",
        "\n",
        "# Solve; if still singular (rare), fall back to least-squares\n",
        "try:\n",
        "    V_nonterm = np.linalg.solve(A, b)\n",
        "except np.linalg.LinAlgError:\n",
        "    V_nonterm = np.linalg.lstsq(A, b, rcond=None)[0]\n",
        "\n",
        "V = {s: 0.0 for s in term}\n",
        "for s in nonterm:\n",
        "    V[s] = float(V_nonterm[idx[s]])\n",
        "\n",
        "expected_discomfort = {s: -V[s] for s in states}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er6-0c0f-BGw"
      },
      "source": [
        "Save your results in a file \"minimum-discomfort-values.tsv\" with columns state and expected_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NAQFQnp_-TZ1"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "pd.DataFrame({\n",
        "    \"state\": states,\n",
        "    \"expected_discomfort\": [expected_discomfort[s] for s in states]\n",
        "}).to_csv(\"minimum-discomfort-values.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83wnFZfk-UDd"
      },
      "source": [
        "Submit \"minimum-discomfort-values.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKUTt9gx-XBF"
      },
      "source": [
        "## Part 4: Minimizing Twizzleflu Duration\n",
        "\n",
        "Modifiy the Markov decision process to minimize the days until the Twizzle flu is over.\n",
        "To do so, change the reward function to always be -1 if the current state corresponds to being sick (must have symptoms, exposed does not count) and 0 otherwise.\n",
        "To be clear, the action does not matter for this reward function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HXrnkCh5-trk"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "duration_rewards = rew0.copy()\n",
        "duration_rewards.columns = [c.strip().lower() for c in duration_rewards.columns]\n",
        "\n",
        "def is_sick(state):\n",
        "    x = str(state).lower()\n",
        "    return (\"symptom\" in x) or (\"fever\" in x) or (\"sick\" in x)\n",
        "\n",
        "duration_rewards[\"reward\"] = duration_rewards[\"state\"].apply(lambda s: -1 if is_sick(s) else 0)\n",
        "reward_duration = build_reward_fn(duration_rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je9Rt239-uRl"
      },
      "source": [
        "Save your new reward function in a file \"duration-rewards.tsv\" in the same format as \"twizzleflu-rewards.tsv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_cmV1ewj-4-Q"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "duration_rewards.to_csv(\"duration-rewards.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0lubs9v-5XQ"
      },
      "source": [
        "Submit \"duration-rewards.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf73YFzB-802"
      },
      "source": [
        "## Part 5: Optimize for Shorter Twizzleflu\n",
        "\n",
        "Compute an optimal policy to minimize the duration of Twizzleflu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sa_HI0f0_FHA"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "policy_speed, _ = value_iteration_optimal_policy(reward_duration, gamma=0.99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px1xDndA_F3O"
      },
      "source": [
        "Save the optimal actions for each state to a file \"minimum-duration-actions.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PGvWqSiI_Sqy"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "pd.DataFrame({\"state\": states,\n",
        "              \"action\": [policy_speed.get(s, \"\") for s in states]}\n",
        ").to_csv(\"minimum-duration-actions.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itPVLMaM_UDn"
      },
      "source": [
        "Submit \"minimum-duration-actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOzSQ3fV_XBO"
      },
      "source": [
        "## Part 6: Shorter Twizzleflu?\n",
        "\n",
        "Compute the expected number of days sick for each state to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WO_yubXg_gxn"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "def evaluate_policy_expected_steps(policy_map, step_cost_by_state, tol=1e-10, max_iter=200000):\n",
        "    D = {s: 0.0 for s in states}\n",
        "    for _ in range(max_iter):\n",
        "        delta = 0.0\n",
        "        Dnew = {}\n",
        "        for s in states:\n",
        "            a = policy_map.get(s)\n",
        "            if a is None or (s, a) not in group:\n",
        "                Dnew[s] = D[s]\n",
        "                continue\n",
        "            v = float(step_cost_by_state.get(s, 0.0))\n",
        "            for sp, p in group[(s, a)]:\n",
        "                v += p * D.get(sp, 0.0)\n",
        "            Dnew[s] = v\n",
        "            delta = max(delta, abs(v - D[s]))\n",
        "        D = Dnew\n",
        "        if delta < tol:\n",
        "            break\n",
        "    return D\n",
        "\n",
        "r_map = {str(row[\"state\"]): float(row[\"reward\"]) for _, row in duration_rewards.iterrows()}\n",
        "step_cost = {s: (1.0 if r_map.get(s, 0.0) == -1 else 0.0) for s in states}\n",
        "\n",
        "expected_sick_days = evaluate_policy_expected_steps(policy_speed, step_cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zf8j6_D_hbZ"
      },
      "source": [
        "Save the expected sick days for each state to a file \"minimum-duration-days.tsv\" with columns state and expected_sick_days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yWS2HNVl_o3P"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "pd.DataFrame({\"state\": states,\n",
        "              \"expected_sick_days\": [expected_sick_days[s] for s in states]}\n",
        ").to_csv(\"minimum-duration-days.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVApozXF_pjI"
      },
      "source": [
        "Submit \"minimum-duration-days.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znw87KK7_uv5"
      },
      "source": [
        "## Part 7: Speed vs Pampering\n",
        "\n",
        "Compute the expected discomfort using the policy to minimize days sick, and compare the results to the expected discomfort when optimizing to minimize discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0AdnpD-6__y5"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "V_speed_disc  = evaluate_policy_value(policy_speed, reward_discomfort, gamma=0.99)\n",
        "V_pamper_disc = evaluate_policy_value(policy_discomfort, reward_discomfort, gamma=0.99)\n",
        "\n",
        "speed_discomfort = {s: -V_speed_disc[s] for s in states}\n",
        "pamper_discomfort = {s: -V_pamper_disc[s] for s in states}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ZVJ2lcAAkP"
      },
      "source": [
        "Save the results to a file \"policy-comparison.tsv\" with columns state, speed_discomfort, and minimize_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9H9EG0zTAMt1"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"state\": states,\n",
        "    \"speed_discomfort\": [speed_discomfort[s] for s in states],\n",
        "    \"minimize_discomfort\": [pamper_discomfort[s] for s in states],\n",
        "}).to_csv(\"policy-comparison.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVhLZuuaANNf"
      },
      "source": [
        "Submit \"policy-comparison.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 8: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 9: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
